Trattamenti statistici dei dati testuali (L. Lebart, CNRS-ENST; lebart@enst.fr)
Il materiale statistico « testo » è onnipresente, quasi banale, sin dallo sviluppo di Internet e del web. Lo studio quantitativo e statistico di questi testi sembra essere apparso di recente, eppure gli studi statistici sui testi risalgono a diversi decenni fa, in particolare in Francia con i lavori di P. Guiraud (« Problemi e metodi di statistica linguistica », PUF, 1960), C. Muller (« Principi e metodi di statistica lessicale », Hachette, 1977) e successivamente J.P. Benzécri (« Pratica dell'analisi dei dati, vol. 3: Linguistica e lessicologia », Dunod, 1981).
Dopo la « stilometria », dedicata allo studio della forma dei testi, al fine di identificare un autore o di datare un'opera, sono apparse le tecniche di documentazione automatica (information retrieval in inglese), che mirano a ricercare in un database di documenti (articoli scientifici, riassunti, brevetti, ecc.) gli elementi pertinenti a partire da una richiesta espressa sotto forma di testo libero. Il campo disciplinare « trattamento del linguaggio naturale » è poi emerso e si è sviluppato inizialmente come uno dei campi di applicazione privilegiati dell'intelligenza artificiale. La complessità del materiale, la necessità di assimilare enormi corpus di testi, la rilevanza del concetto di apprendimento hanno naturalmente aperto questo campo ai metodi statistici. La statistica multidimensionale, le catene di Markov nascoste, i metodi di analisi discriminante intervengono quindi per costruire gli strumenti di base che sono i motori di ricerca sul web, gli analizzatori morfosintattici, i correttori ortografici, nonché in campi applicativi pratici come il trattamento delle risposte alle domande aperte nelle indagini socio-economiche.
Le domande aperte
In un certo numero di situazioni di indagine, è utile lasciare aperte alcune domande, le cui risposte si presenteranno quindi sotto forma di testi di lunghezza variabile.
La raccolta dei dati
In almeno tre situazioni comuni, l'uso di domande aperte è necessario:
Per ridurre o ottimizzare la durata dell'intervista di indagine: Sebbene le risposte libere e quelle guidate forniscano informazioni di natura diversa, le prime sono più economiche in termini di tempo di intervista e generano meno stanchezza. Una semplice domanda aperta (ad esempio: « Quali sono state le tue principali attività domenica scorsa? ») può sostituire lunghe liste di elementi.
Come complemento a domande chiuse: Di solito si tratta della domanda « Perché? ». Le spiegazioni riguardanti una risposta già data devono necessariamente essere spontanee. Un elenco di elementi potrebbe suggerire nuovi argomenti che potrebbero compromettere l'autenticità dell'argomentazione.
Per raccogliere informazioni che devono essere spontanee per loro natura: I questionari delle indagini di marketing abbondano di domande di questo tipo. Esempi includono: « Cosa ricordi di questa campagna pubblicitaria? » oppure « Cosa pensi di questa auto? ».
Unità statistiche
I programmi lavorano a partire dal testo grezzo, estraendo automaticamente delle unità statistiche, per lo più forme grafiche (sequenze di caratteri non separatori). Si usa il termine forma grafica perché la parola « parola » è ambigua. Può infatti riferirsi all'occorrenza di una parola, al tipo, oppure al lemma (ad esempio, « avere » è il lemma di « aveva »).
nel caso dell’esempio precedente per 1009 risposte si ottengono 14337 occorrenze di 1394 forme distinte (o tipi) è ben noto che la distribuzione di frequenza delle parole è molto asimmetrica (legge di zipf, simile alla distribuzione di pareto) così selezionando solo le forme che appaiono almeno 20 volte rimane un testo di 10994 forme con solo 97 forme distinte (così il 7% delle parole distinte corrisponde al 77% del testo totale) in particolare quasi la metà delle forme grafiche distinte appare una sola volta (queste sono gli « hapax »)
il post-codifica
il pretrattamento empirico chiamato « post-codifica » permette di chiudere a posteriori le domande aperte questa tecnica comune consiste nel costruire una serie di elementi a partire da un sotto-campione di risposte per poi codificare tutte le risposte in modo da sostituire la domanda aperta con una o più domande chiuse per l’esempio sopra la seconda risposta la più semplice darebbe gli elementi « lettura » « viaggi » « tempo libero » a condizione che questi elementi appaiano con una certa frequenza nel campione di risposte tuttavia la prima risposta è più difficile da post-codificare
gli strumenti statistici di base
gli strumenti di base comprendono la selezione di forme caratteristiche la selezione di risposte modali l'analisi delle corrispondenze e la classificazione delle tabelle lessicali
forme o segmenti caratteristici (o specificità)
le forme caratteristiche sono le forme « anormalmente » frequenti nelle risposte di un gruppo di individui (tecnica proposta da p lafon nel 1980) un test elementare basato sulla legge ipergeometrica permette di selezionare le parole (forme grafiche o lemmi) la cui frequenza in un gruppo è significativamente superiore (o inferiore per le parole anti-caratteristiche) alla frequenza media nel corpus si tratta di test classici di confronto delle frequenze ma la ripetizione di questo test porta a prendere soglie di significatività molto rigide (fenomeno di confronti multipli ben noto agli statistici)
nell’esempio citato sopra la frequenza media della parola lavoro nel corpus era del 3,4%; per il gruppo delle donne oltre i 55 anni la frequenza è solo dell’1,2% questa differenza è altamente significativa (si può esprimere il test di confronto delle frequenze in termini di scarti standard nella ipotesi di omogeneità delle frequenze il valore del 1,2% è a 4,5 scarti standard dal valore medio del 3,4%) poiché si tratta di una frequenza anormalmente bassa si parlerà di parole anti-caratteristiche
le selezioni delle risposte modali
per un gruppo di individui e quindi per il raggruppamento delle risposte corrispondenti le risposte modali (o frasi caratteristiche o documenti-tipo la terminologia varia a seconda dei campi di applicazione) sono risposte originali del corpus di base che caratterizzano meglio il gruppo si può per ogni raggruppamento calcolare la distanza del profilo lessicale di un individuo dal profilo lessicale medio del gruppo poi si possono ordinare le distanze in ordine crescente e selezionare le risposte più rappresentative in termini di profilo lessicale che corrisponderanno alle distanze minori si ottiene così una sorta di sintesi delle risposte di ogni gruppo costituita da risposte originali (l lebart e a salem statistica testuale dunod 1994) sempre nel caso del nostro esempio « essere felice avere un buon lavoro successo professionale e familiare » è una risposta caratteristica dei giovani uomini « la salute la famiglia » è una risposta che caratterizza le persone più anziane in pratica si utilizzano più risposte caratteristiche per ogni gruppo
analisi delle corrispondenze e classificazione
il volume dei dati richiede l'uso di potenti strumenti di descrizione i metodi di analisi delle corrispondenze e di classificazione possono descrivere le tabelle di contingenza che incrociano le risposte con le forme grafiche o gruppi di risposte (ad esempio raggruppamenti in base al livello di istruzione dei rispondenti) e le forme grafiche questi strumenti permettono di visualizzare sotto forma di serie di mappe piane (o dendrogrammi nel caso dei metodi di classificazione o mappe auto-organizzate di kohonen metodo « neurale » di visualizzazione) le associazioni tra parole (forme) e gruppi o modalità così una visualizzazione delle prossimità tra parole e categorie socio-professionali può aiutare a leggere le risposte di ciascuna di queste categorie
conclusioni e prospettive
per risposte semplici e stereotipate come abbiamo visto le procedure di post-codifica possono funzionare tuttavia tra i difetti di questo tipo di trattamento si possono menzionare:
la mediazione del codificatore: le decisioni da prendere sono talvolta difficili
la qualità dell'espressione il registro del vocabolario la tonalità generale dell'intervista sono elementi di analisi persi durante la post-codifica (bisogna codificare in modo diverso « non lo so » e « preferisco non dire nulla »?)
le risposte composite complesse e molto diverse sono difficili da post-codificare ed è spesso in questi casi che il valore euristico delle risposte libere è maggiore
le risposte poco frequenti originali e poco chiare a una prima lettura sono considerate come « rumore » e assegnate a categorie residuali (« altre ») che sono quindi molto eterogenee e difficili da gestire senza che sia necessario procedere a una post-codifica attualmente è possibile a partire da un insieme di testi e da una soglia di frequenza per le forme grafiche ottenere una visualizzazione delle prossimità tra testi in base ai loro profili lessicali e tra forme grafiche in base alla loro distribuzione nei testi l'arricchimento delle unità statistiche con segmenti ripetuti cf a salem pratica dei segmenti ripetuti klincksieck 1987 i loro raggruppamenti per categorizzazione morfologica l'utilizzo delle forme caratteristiche o specificità l'aggiunta delle risposte modali o delle frasi o unità di contesto caratteristiche hanno perfezionato questi approcci e messo a disposizione di molti utenti metodi e software utili in alcuni specifici ambiti applicativi come il trattamento automatico delle risposte alle domande aperte che ci interessa qui l'efficacia del metodo come complemento alle approcci tradizionali è riconosciuta parallelamente ai lavori dell'industria della lingua che abbiamo menzionato in precedenza e che fanno parte di un'ingegneria statistica complessa esistono quindi applicazioni testuali della statistica a portata di mano richiedono sicuramente software specifici ma la natura familiare e viva del materiale di base compensa in qualche modo la relativa complessità dei trattamenti e le difficoltà di interpretazione vicino alle basi di dati all'intelligenza artificiale e alle reti neurali alla teoria dell'apprendimento alle tecniche recenti di estrazione e gestione della conoscenza il dominio testuale illustra bene la polivalenza e la potenza della metodologia statistica anche quando i metodi assumono nomi più esotici come text mining o text mining il lavoro dello statistico è sempre necessario quando si tratta di conoscere la portata reale dei fatti osservati e dei tratti strutturali ottenuti di sapere cosa si può affermare e cosa non si deve dire ovvero di dare uno statuto scientifico ai risultati
